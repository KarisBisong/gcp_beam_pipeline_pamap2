{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPF2sQTuXjKO"
   },
   "source": [
    "# Big Data Pipelines with Apache Beam on Google Cloud DataFlow\n",
    "## by Karis Bisong (Cloud Big Data Engineer)\n",
    "\n",
    "Table of contents:\n",
    "\n",
    "- [Create a bucket on GCS](#bucket-gcs)\n",
    "- [Ephemeral GCE instance for data staging](#ephmeral-gce)\n",
    "- [Download from data source to ephemeral GCE](#data-download)\n",
    "- [Transfer data from transient GCE to Google Cloud Storage (GCS)](#transfer-data)\n",
    "- [Exploratory Data Analysis](#eda)\n",
    "- [Create BigQuery Dataset](#bq-dataset)\n",
    "- [Big Data Pipeline with Apache Beam](#apache-beam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ar5oiev1QbV-"
   },
   "source": [
    "![Pipeline Overview.](big-data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u61Yz3zuakDe"
   },
   "source": [
    "<a name=\"#bucket-gcs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFmy4JfBXjKP"
   },
   "source": [
    "# Create a bucket on GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8PmKoNIXjKQ"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAeulPdHXjKU"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "bucket_name = 'kbisong-blessings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQZi7shxXjKX"
   },
   "outputs": [],
   "source": [
    "def create_bucket_class_location(bucket_name):\n",
    "    \"\"\"\"Create a new bucket in specific location with storage class\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    bucket.storage_class = \"STANDARD\"\n",
    "    new_bucket = storage_client.create_bucket(bucket, location=\"us\")\n",
    "    \n",
    "    print(\n",
    "        \"Created bucket {} in {} with storage class {}\".format(\n",
    "            new_bucket.name, new_bucket.location, new_bucket.storage_class\n",
    "         )\n",
    "    )\n",
    "    return new_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hocSTq47XjKa",
    "outputId": "d489a023-729b-4d89-ec27-99eeeca6d0a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bucket kbisong-blessings in US with storage class STANDARD\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Bucket: kbisong-blessings>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_bucket_class_location(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgwN9fvmbPxg"
   },
   "source": [
    "<a name=\"#ephmeral-gce\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5hIJAo1bNWv"
   },
   "source": [
    "# Ephemeral GCE instance for data staging\n",
    "\n",
    "* Run the code in this section on the shell. \n",
    "* If using a notebook cell on Colab you may uncomment and run as it.\n",
    "* Otherwise on Jupyter notebook runnng on a Cloud VM or locally, run the code on your local terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQHCy1lBXjKe"
   },
   "source": [
    "## Get Aplication Default Credentials (ADC) credentials to authenticate host VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsQw1Vv3XjKf"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# run the below code in a terminal\n",
    "gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWfrbpiwXjKi"
   },
   "source": [
    "## Create GCE instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiX1o_qHXjKj",
    "outputId": "c3f79af2-2810-4bb4-ca3c-dc69ba40f230"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                  ZONE           MACHINE_TYPE  PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP    STATUS\n",
      "kbisong-data-staging  us-central1-a  e2-medium                  10.128.0.2   35.223.16.145  RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created [https://www.googleapis.com/compute/v1/projects/qualified-root-293322/zones/us-central1-a/instances/kbisong-data-staging].\n",
      "WARNING: Some requests generated warnings:\n",
      " - Disk size: '300 GB' is larger than image size: '50 GB'. You might need to resize the root repartition manually if the operating system does not support automatic resizing. See https://cloud.google.com/compute/docs/disks/add-persistent-disk#resize_pd for details.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# create GCE instance      \n",
    "INSTANCE_NAME=\"kbisong-data-staging\"\n",
    "ZONE=us-central1-a\n",
    "\n",
    "gcloud compute instances create $INSTANCE_NAME \\\n",
    "     --image-project='deeplearning-platform-release' \\\n",
    "     --image-family='tf2-2-0-cu100' \\\n",
    "     --machine-type=e2-medium \\\n",
    "     --scopes='https://www.googleapis.com/auth/cloud-platform' \\\n",
    "     --zone=$ZONE \\\n",
    "     --tags=http-server \\\n",
    "     --boot-disk-size='300GB'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FxZp-5mcwsL"
   },
   "source": [
    "<a name=\"#data-download\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Up1AooSqXjKe"
   },
   "source": [
    "# Download from data source to ephemeral GCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsFVXdLoba0_"
   },
   "source": [
    "* The dataset is the PAMAP2 Physical Activity Monitoring Data Set from the UCI Machine Learning repository.\n",
    "* The dataset is available at: [https://archive.ics.uci.edu/ml/datasets/pamap2+physical+activity+monitoring](https://archive.ics.uci.edu/ml/datasets/pamap2+physical+activity+monitoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ws-jPDctXjKo"
   },
   "source": [
    "### ssh to VM from the terminal\n",
    "* **Note:** Replace the `zone`, `instance-name` and `project` appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJ8iMlOKXjKo"
   },
   "outputs": [],
   "source": [
    "# ssh to VM from the terminal.\n",
    "gcloud beta compute ssh --zone \"us-central1-a\" \"kbisong-data-staging\" --project \"qualified-root-293322\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fQG7yC5XjKr"
   },
   "source": [
    "### Download data from UCI repository to GCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JYMnxJgXjKs"
   },
   "outputs": [],
   "source": [
    "# Use Curl to transfer Zip file from UCI repository to the ephemeral Compute Engine\n",
    "curl https://archive.ics.uci.edu/ml/machine-learning-databases/00231/PAMAP2_Dataset.zip --output PAMAP2_Dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KP6Z2IPPdv2f"
   },
   "source": [
    "<a name=\"#transfer-data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYiRv3yxXjKv"
   },
   "source": [
    "# Transfer data from transient GCE to Google Cloud Storage (GCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QDfBmINXjKw"
   },
   "outputs": [],
   "source": [
    "# run the gsutil command on the terminal to transfer from the VM to GCS\n",
    "gsutil cp -r PAMAP2_Dataset gs://kbisong-blessings/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSgawhUKXjK2"
   },
   "source": [
    "### Clean-up staging GCE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVfpjpMuXjK3"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# delete GCE instance\n",
    "INSTANCE_NAME=\"kbisong-data-staging\"\n",
    "ZONE=us-central1-a\n",
    "\n",
    "gcloud compute instances delete $INSTANCE_NAME --zone=$ZONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RoU584wDhnJ"
   },
   "source": [
    "<a name=\"#eda\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tu1Sa2-XjLA"
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfv0Ra1KXjLB"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4HHHZYxmXjLE"
   },
   "outputs": [],
   "source": [
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 2000)\n",
    "    pd.set_option('display.float_format', '{:20,.2f}'.format)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "    pd.reset_option('display.max_columns')\n",
    "    pd.reset_option('display.width')\n",
    "    pd.reset_option('display.float_format')\n",
    "    pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XbAeskwXjLH"
   },
   "source": [
    "### Load a sample .dat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UeF2ixuvXjLH"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gcs_path = 'gs://kbisong-blessings/PAMAP2_Dataset/Optional/subject106.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wC1-9ObXjLJ"
   },
   "outputs": [],
   "source": [
    "# create column names\n",
    "imu_hand_list = ['_'.join(('imu_hand',str(i))) for i in range(4,21)]\n",
    "imu_chest_list = ['_'.join(('imu_chest',str(i))) for i in range(21,38)]\n",
    "imu_ankle_list = ['_'.join(('imu_ankle',str(i))) for i in range(38,55)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeqS9LIgXjLM"
   },
   "outputs": [],
   "source": [
    "columns = ['timestamp','activity_id','heart_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kH4WrJ_WXjLO"
   },
   "outputs": [],
   "source": [
    "columns.extend(imu_hand_list)\n",
    "columns.extend(imu_chest_list)\n",
    "columns.extend(imu_ankle_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sA9pSxnVXjLQ",
    "outputId": "c43bdbcd-5107-45f1-90a9-9b395e91694e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBSJZYhGXjLT"
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = pd.read_csv(gcs_path, sep=' ', names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIo2A8H0XjLV",
    "outputId": "c835c55c-3602-4a1c-9d64-ea31e967fc61"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>activity_id</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>imu_hand_4</th>\n",
       "      <th>imu_hand_5</th>\n",
       "      <th>imu_hand_6</th>\n",
       "      <th>imu_hand_7</th>\n",
       "      <th>imu_hand_8</th>\n",
       "      <th>imu_hand_9</th>\n",
       "      <th>imu_hand_10</th>\n",
       "      <th>...</th>\n",
       "      <th>imu_ankle_45</th>\n",
       "      <th>imu_ankle_46</th>\n",
       "      <th>imu_ankle_47</th>\n",
       "      <th>imu_ankle_48</th>\n",
       "      <th>imu_ankle_49</th>\n",
       "      <th>imu_ankle_50</th>\n",
       "      <th>imu_ankle_51</th>\n",
       "      <th>imu_ankle_52</th>\n",
       "      <th>imu_ankle_53</th>\n",
       "      <th>imu_ankle_54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.93</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.25</td>\n",
       "      <td>-7.60142</td>\n",
       "      <td>-0.363243</td>\n",
       "      <td>6.39334</td>\n",
       "      <td>-7.57372</td>\n",
       "      <td>-0.300108</td>\n",
       "      <td>6.42000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077609</td>\n",
       "      <td>0.011736</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>-46.5229</td>\n",
       "      <td>3.20015</td>\n",
       "      <td>44.7834</td>\n",
       "      <td>0.023780</td>\n",
       "      <td>-0.728393</td>\n",
       "      <td>0.352740</td>\n",
       "      <td>-0.586900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.94</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.25</td>\n",
       "      <td>-7.56230</td>\n",
       "      <td>-0.363199</td>\n",
       "      <td>6.43241</td>\n",
       "      <td>-7.65022</td>\n",
       "      <td>-0.435476</td>\n",
       "      <td>6.42038</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046013</td>\n",
       "      <td>0.012430</td>\n",
       "      <td>0.019330</td>\n",
       "      <td>-46.3904</td>\n",
       "      <td>3.19480</td>\n",
       "      <td>45.2093</td>\n",
       "      <td>0.023821</td>\n",
       "      <td>-0.728416</td>\n",
       "      <td>0.353048</td>\n",
       "      <td>-0.586685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.95</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.25</td>\n",
       "      <td>-7.60276</td>\n",
       "      <td>-0.363578</td>\n",
       "      <td>6.35483</td>\n",
       "      <td>-7.63464</td>\n",
       "      <td>-0.405356</td>\n",
       "      <td>6.45050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124982</td>\n",
       "      <td>-0.007319</td>\n",
       "      <td>0.034692</td>\n",
       "      <td>-46.3885</td>\n",
       "      <td>2.80679</td>\n",
       "      <td>44.6425</td>\n",
       "      <td>0.024607</td>\n",
       "      <td>-0.727628</td>\n",
       "      <td>0.354943</td>\n",
       "      <td>-0.586488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.96</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.25</td>\n",
       "      <td>-7.51568</td>\n",
       "      <td>-0.247661</td>\n",
       "      <td>6.66364</td>\n",
       "      <td>-7.63401</td>\n",
       "      <td>-0.390201</td>\n",
       "      <td>6.51086</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108404</td>\n",
       "      <td>-0.023163</td>\n",
       "      <td>0.051130</td>\n",
       "      <td>-46.7838</td>\n",
       "      <td>3.47065</td>\n",
       "      <td>44.7821</td>\n",
       "      <td>0.024698</td>\n",
       "      <td>-0.727041</td>\n",
       "      <td>0.356218</td>\n",
       "      <td>-0.586439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.97</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.25</td>\n",
       "      <td>-7.56791</td>\n",
       "      <td>-0.213119</td>\n",
       "      <td>6.23938</td>\n",
       "      <td>-7.63351</td>\n",
       "      <td>-0.344884</td>\n",
       "      <td>6.52587</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047585</td>\n",
       "      <td>-0.010232</td>\n",
       "      <td>0.022264</td>\n",
       "      <td>-46.5084</td>\n",
       "      <td>2.81487</td>\n",
       "      <td>45.3512</td>\n",
       "      <td>0.024759</td>\n",
       "      <td>-0.726497</td>\n",
       "      <td>0.357154</td>\n",
       "      <td>-0.586541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  activity_id  heart_rate  imu_hand_4  imu_hand_5  imu_hand_6  \\\n",
       "0       5.93            0         NaN       28.25    -7.60142   -0.363243   \n",
       "1       5.94            0         NaN       28.25    -7.56230   -0.363199   \n",
       "2       5.95            0         NaN       28.25    -7.60276   -0.363578   \n",
       "3       5.96            0         NaN       28.25    -7.51568   -0.247661   \n",
       "4       5.97            0         NaN       28.25    -7.56791   -0.213119   \n",
       "\n",
       "   imu_hand_7  imu_hand_8  imu_hand_9  imu_hand_10  ...  imu_ankle_45  \\\n",
       "0     6.39334    -7.57372   -0.300108      6.42000  ...     -0.077609   \n",
       "1     6.43241    -7.65022   -0.435476      6.42038  ...     -0.046013   \n",
       "2     6.35483    -7.63464   -0.405356      6.45050  ...     -0.124982   \n",
       "3     6.66364    -7.63401   -0.390201      6.51086  ...     -0.108404   \n",
       "4     6.23938    -7.63351   -0.344884      6.52587  ...     -0.047585   \n",
       "\n",
       "   imu_ankle_46  imu_ankle_47  imu_ankle_48  imu_ankle_49  imu_ankle_50  \\\n",
       "0      0.011736      0.001910      -46.5229       3.20015       44.7834   \n",
       "1      0.012430      0.019330      -46.3904       3.19480       45.2093   \n",
       "2     -0.007319      0.034692      -46.3885       2.80679       44.6425   \n",
       "3     -0.023163      0.051130      -46.7838       3.47065       44.7821   \n",
       "4     -0.010232      0.022264      -46.5084       2.81487       45.3512   \n",
       "\n",
       "   imu_ankle_51  imu_ankle_52  imu_ankle_53  imu_ankle_54  \n",
       "0      0.023780     -0.728393      0.352740     -0.586900  \n",
       "1      0.023821     -0.728416      0.353048     -0.586685  \n",
       "2      0.024607     -0.727628      0.354943     -0.586488  \n",
       "3      0.024698     -0.727041      0.356218     -0.586439  \n",
       "4      0.024759     -0.726497      0.357154     -0.586541  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vf6F2Xk5XjLX"
   },
   "source": [
    "### Data summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7aHRIiNSXjLY",
    "outputId": "c119534d-80f1-44b3-d4a6-14fc58ce9f6e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>activity_id</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>imu_hand_4</th>\n",
       "      <th>imu_hand_5</th>\n",
       "      <th>imu_hand_6</th>\n",
       "      <th>imu_hand_7</th>\n",
       "      <th>imu_hand_8</th>\n",
       "      <th>imu_hand_9</th>\n",
       "      <th>imu_hand_10</th>\n",
       "      <th>...</th>\n",
       "      <th>imu_ankle_45</th>\n",
       "      <th>imu_ankle_46</th>\n",
       "      <th>imu_ankle_47</th>\n",
       "      <th>imu_ankle_48</th>\n",
       "      <th>imu_ankle_49</th>\n",
       "      <th>imu_ankle_50</th>\n",
       "      <th>imu_ankle_51</th>\n",
       "      <th>imu_ankle_52</th>\n",
       "      <th>imu_ankle_53</th>\n",
       "      <th>imu_ankle_54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>129963.000000</td>\n",
       "      <td>129963.000000</td>\n",
       "      <td>11856.00000</td>\n",
       "      <td>129897.000000</td>\n",
       "      <td>129897.000000</td>\n",
       "      <td>129897.000000</td>\n",
       "      <td>129897.000000</td>\n",
       "      <td>129897.000000</td>\n",
       "      <td>129897.000000</td>\n",
       "      <td>129897.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>129765.000000</td>\n",
       "      <td>129765.000000</td>\n",
       "      <td>129765.000000</td>\n",
       "      <td>129765.000000</td>\n",
       "      <td>129765.000000</td>\n",
       "      <td>129765.000000</td>\n",
       "      <td>1.297650e+05</td>\n",
       "      <td>129765.000000</td>\n",
       "      <td>129765.000000</td>\n",
       "      <td>129765.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>655.740000</td>\n",
       "      <td>11.968668</td>\n",
       "      <td>84.04698</td>\n",
       "      <td>31.874510</td>\n",
       "      <td>-1.916314</td>\n",
       "      <td>3.728037</td>\n",
       "      <td>5.690505</td>\n",
       "      <td>-1.872209</td>\n",
       "      <td>3.752566</td>\n",
       "      <td>5.864998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020447</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>-28.199348</td>\n",
       "      <td>7.193297</td>\n",
       "      <td>5.634536</td>\n",
       "      <td>4.241375e-01</td>\n",
       "      <td>0.183533</td>\n",
       "      <td>0.481809</td>\n",
       "      <td>0.113022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>375.172309</td>\n",
       "      <td>6.193361</td>\n",
       "      <td>11.64302</td>\n",
       "      <td>1.281535</td>\n",
       "      <td>4.876080</td>\n",
       "      <td>3.525859</td>\n",
       "      <td>3.941986</td>\n",
       "      <td>4.879370</td>\n",
       "      <td>3.525739</td>\n",
       "      <td>3.903460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.592971</td>\n",
       "      <td>0.228879</td>\n",
       "      <td>0.634977</td>\n",
       "      <td>11.806593</td>\n",
       "      <td>16.044235</td>\n",
       "      <td>32.510547</td>\n",
       "      <td>2.388896e-01</td>\n",
       "      <td>0.482827</td>\n",
       "      <td>0.313118</td>\n",
       "      <td>0.391510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.930000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>65.00000</td>\n",
       "      <td>28.250000</td>\n",
       "      <td>-51.863200</td>\n",
       "      <td>-33.582700</td>\n",
       "      <td>-19.134000</td>\n",
       "      <td>-41.427800</td>\n",
       "      <td>-25.935800</td>\n",
       "      <td>-16.450000</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.559470</td>\n",
       "      <td>-2.845290</td>\n",
       "      <td>-5.503420</td>\n",
       "      <td>-80.626800</td>\n",
       "      <td>-140.460000</td>\n",
       "      <td>-103.905000</td>\n",
       "      <td>9.350400e-07</td>\n",
       "      <td>-0.856342</td>\n",
       "      <td>-0.588061</td>\n",
       "      <td>-0.714273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>330.835000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>73.00000</td>\n",
       "      <td>31.062500</td>\n",
       "      <td>-5.489680</td>\n",
       "      <td>1.240580</td>\n",
       "      <td>2.787900</td>\n",
       "      <td>-5.434060</td>\n",
       "      <td>1.230000</td>\n",
       "      <td>3.001720</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023360</td>\n",
       "      <td>-0.030204</td>\n",
       "      <td>-0.021337</td>\n",
       "      <td>-36.146500</td>\n",
       "      <td>2.209810</td>\n",
       "      <td>-24.648500</td>\n",
       "      <td>1.919980e-01</td>\n",
       "      <td>0.033228</td>\n",
       "      <td>0.196681</td>\n",
       "      <td>-0.051621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>655.740000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>86.00000</td>\n",
       "      <td>32.437500</td>\n",
       "      <td>-0.802599</td>\n",
       "      <td>3.061010</td>\n",
       "      <td>6.394860</td>\n",
       "      <td>-0.800262</td>\n",
       "      <td>3.127130</td>\n",
       "      <td>6.616820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006917</td>\n",
       "      <td>-0.004946</td>\n",
       "      <td>0.001437</td>\n",
       "      <td>-28.064000</td>\n",
       "      <td>10.036900</td>\n",
       "      <td>-12.399700</td>\n",
       "      <td>5.396580e-01</td>\n",
       "      <td>0.141850</td>\n",
       "      <td>0.682339</td>\n",
       "      <td>0.031029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>980.645000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>94.00000</td>\n",
       "      <td>32.875000</td>\n",
       "      <td>0.056180</td>\n",
       "      <td>6.255460</td>\n",
       "      <td>9.255810</td>\n",
       "      <td>0.145979</td>\n",
       "      <td>6.271760</td>\n",
       "      <td>9.375310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035525</td>\n",
       "      <td>0.020922</td>\n",
       "      <td>0.022447</td>\n",
       "      <td>-26.219600</td>\n",
       "      <td>17.238000</td>\n",
       "      <td>38.632300</td>\n",
       "      <td>6.397950e-01</td>\n",
       "      <td>0.689745</td>\n",
       "      <td>0.753183</td>\n",
       "      <td>0.553119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1305.550000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>109.00000</td>\n",
       "      <td>33.250000</td>\n",
       "      <td>22.024500</td>\n",
       "      <td>48.930500</td>\n",
       "      <td>75.463200</td>\n",
       "      <td>21.405900</td>\n",
       "      <td>49.332400</td>\n",
       "      <td>52.657200</td>\n",
       "      <td>...</td>\n",
       "      <td>9.555860</td>\n",
       "      <td>3.320420</td>\n",
       "      <td>7.310840</td>\n",
       "      <td>22.271000</td>\n",
       "      <td>67.698100</td>\n",
       "      <td>104.700000</td>\n",
       "      <td>8.021700e-01</td>\n",
       "      <td>0.847623</td>\n",
       "      <td>0.866597</td>\n",
       "      <td>0.716415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           timestamp    activity_id   heart_rate     imu_hand_4  \\\n",
       "count  129963.000000  129963.000000  11856.00000  129897.000000   \n",
       "mean      655.740000      11.968668     84.04698      31.874510   \n",
       "std       375.172309       6.193361     11.64302       1.281535   \n",
       "min         5.930000       0.000000     65.00000      28.250000   \n",
       "25%       330.835000      10.000000     73.00000      31.062500   \n",
       "50%       655.740000      10.000000     86.00000      32.437500   \n",
       "75%       980.645000      18.000000     94.00000      32.875000   \n",
       "max      1305.550000      19.000000    109.00000      33.250000   \n",
       "\n",
       "          imu_hand_5     imu_hand_6     imu_hand_7     imu_hand_8  \\\n",
       "count  129897.000000  129897.000000  129897.000000  129897.000000   \n",
       "mean       -1.916314       3.728037       5.690505      -1.872209   \n",
       "std         4.876080       3.525859       3.941986       4.879370   \n",
       "min       -51.863200     -33.582700     -19.134000     -41.427800   \n",
       "25%        -5.489680       1.240580       2.787900      -5.434060   \n",
       "50%        -0.802599       3.061010       6.394860      -0.800262   \n",
       "75%         0.056180       6.255460       9.255810       0.145979   \n",
       "max        22.024500      48.930500      75.463200      21.405900   \n",
       "\n",
       "          imu_hand_9    imu_hand_10  ...   imu_ankle_45   imu_ankle_46  \\\n",
       "count  129897.000000  129897.000000  ...  129765.000000  129765.000000   \n",
       "mean        3.752566       5.864998  ...       0.020447       0.000141   \n",
       "std         3.525739       3.903460  ...       0.592971       0.228879   \n",
       "min       -25.935800     -16.450000  ...      -8.559470      -2.845290   \n",
       "25%         1.230000       3.001720  ...      -0.023360      -0.030204   \n",
       "50%         3.127130       6.616820  ...       0.006917      -0.004946   \n",
       "75%         6.271760       9.375310  ...       0.035525       0.020922   \n",
       "max        49.332400      52.657200  ...       9.555860       3.320420   \n",
       "\n",
       "        imu_ankle_47   imu_ankle_48   imu_ankle_49   imu_ankle_50  \\\n",
       "count  129765.000000  129765.000000  129765.000000  129765.000000   \n",
       "mean        0.004171     -28.199348       7.193297       5.634536   \n",
       "std         0.634977      11.806593      16.044235      32.510547   \n",
       "min        -5.503420     -80.626800    -140.460000    -103.905000   \n",
       "25%        -0.021337     -36.146500       2.209810     -24.648500   \n",
       "50%         0.001437     -28.064000      10.036900     -12.399700   \n",
       "75%         0.022447     -26.219600      17.238000      38.632300   \n",
       "max         7.310840      22.271000      67.698100     104.700000   \n",
       "\n",
       "       imu_ankle_51   imu_ankle_52   imu_ankle_53   imu_ankle_54  \n",
       "count  1.297650e+05  129765.000000  129765.000000  129765.000000  \n",
       "mean   4.241375e-01       0.183533       0.481809       0.113022  \n",
       "std    2.388896e-01       0.482827       0.313118       0.391510  \n",
       "min    9.350400e-07      -0.856342      -0.588061      -0.714273  \n",
       "25%    1.919980e-01       0.033228       0.196681      -0.051621  \n",
       "50%    5.396580e-01       0.141850       0.682339       0.031029  \n",
       "75%    6.397950e-01       0.689745       0.753183       0.553119  \n",
       "max    8.021700e-01       0.847623       0.866597       0.716415  \n",
       "\n",
       "[8 rows x 54 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data summaries\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sen-6rZkXjLa",
    "outputId": "e88e0b1e-aee0-467e-b440-fe049bc2f548"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp            0\n",
       "activity_id          0\n",
       "heart_rate      118107\n",
       "imu_hand_4          66\n",
       "imu_hand_5          66\n",
       "imu_hand_6          66\n",
       "imu_hand_7          66\n",
       "imu_hand_8          66\n",
       "imu_hand_9          66\n",
       "imu_hand_10         66\n",
       "imu_hand_11         66\n",
       "imu_hand_12         66\n",
       "imu_hand_13         66\n",
       "imu_hand_14         66\n",
       "imu_hand_15         66\n",
       "imu_hand_16         66\n",
       "imu_hand_17         66\n",
       "imu_hand_18         66\n",
       "imu_hand_19         66\n",
       "imu_hand_20         66\n",
       "imu_chest_21        80\n",
       "imu_chest_22        80\n",
       "imu_chest_23        80\n",
       "imu_chest_24        80\n",
       "imu_chest_25        80\n",
       "imu_chest_26        80\n",
       "imu_chest_27        80\n",
       "imu_chest_28        80\n",
       "imu_chest_29        80\n",
       "imu_chest_30        80\n",
       "imu_chest_31        80\n",
       "imu_chest_32        80\n",
       "imu_chest_33        80\n",
       "imu_chest_34        80\n",
       "imu_chest_35        80\n",
       "imu_chest_36        80\n",
       "imu_chest_37        80\n",
       "imu_ankle_38       198\n",
       "imu_ankle_39       198\n",
       "imu_ankle_40       198\n",
       "imu_ankle_41       198\n",
       "imu_ankle_42       198\n",
       "imu_ankle_43       198\n",
       "imu_ankle_44       198\n",
       "imu_ankle_45       198\n",
       "imu_ankle_46       198\n",
       "imu_ankle_47       198\n",
       "imu_ankle_48       198\n",
       "imu_ankle_49       198\n",
       "imu_ankle_50       198\n",
       "imu_ankle_51       198\n",
       "imu_ankle_52       198\n",
       "imu_ankle_53       198\n",
       "imu_ankle_54       198\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values\n",
    "len(data) - data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26OAG8CzEnVU"
   },
   "source": [
    "<a name=\"#bq-dataset\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHzergaEXjLc"
   },
   "source": [
    "# Big Data Pipeline with Apache Beam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_cezNKxRUxd"
   },
   "source": [
    "Migrate dataset from Google Cloud Storage (GCS) to BigQuery. The original dataset on https://archive.ics.uci.edu/ml/datasets/pamap2+physical+activity+monitoring contains 1 data file per subject per session (protocol or optional).\n",
    "\n",
    "The session `Protocol` contains 9 data files, while `Optional` has 5 data files. The goal of the pipeline is to merge all the data files into their respective sessions as a table in BigQuery.\n",
    "\n",
    "The output on BigQuery contains 2 tables (`Prorocol` and `Optional`), with their associated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYo8sRe4XjLd"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfQgTZxLXjLf"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "table_name='subject106'\n",
    "dataset='PAMAP'\n",
    "project='qualified-root-293322'\n",
    "source_input = 'gs://kbisong-blessings/PAMAP2_Dataset/Optional/subject106.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IrUpnePBXjLi"
   },
   "outputs": [],
   "source": [
    "# BQ schema field\n",
    "df_schema = [bigquery.SchemaField('timestamp', 'STRING'), bigquery.SchemaField('activity_id', 'STRING'),\n",
    " bigquery.SchemaField('heart_rate','STRING'), bigquery.SchemaField('imu_hand_4','STRING'),\n",
    " bigquery.SchemaField('imu_hand_5','STRING'), bigquery.SchemaField('imu_hand_6','STRING'),\n",
    " bigquery.SchemaField('imu_hand_7','STRING'), bigquery.SchemaField('imu_hand_8','STRING'),\n",
    " bigquery.SchemaField('imu_hand_9','STRING'), bigquery.SchemaField('imu_hand_10','STRING'),\n",
    " bigquery.SchemaField('imu_hand_11','STRING'), bigquery.SchemaField('imu_hand_12','STRING'),\n",
    " bigquery.SchemaField('imu_hand_13','STRING'), bigquery.SchemaField('imu_hand_14','STRING'),\n",
    " bigquery.SchemaField('imu_hand_15','STRING'), bigquery.SchemaField('imu_hand_16','STRING'),\n",
    " bigquery.SchemaField('imu_hand_17','STRING'), bigquery.SchemaField('imu_hand_18','STRING'),\n",
    " bigquery.SchemaField('imu_hand_19','STRING'), bigquery.SchemaField('imu_hand_20','STRING'),\n",
    " bigquery.SchemaField('imu_chest_21','STRING'), bigquery.SchemaField('imu_chest_22','STRING'),\n",
    " bigquery.SchemaField('imu_chest_23','STRING'), bigquery.SchemaField('imu_chest_24','STRING'),\n",
    " bigquery.SchemaField('imu_chest_25','STRING'), bigquery.SchemaField('imu_chest_26','STRING'),\n",
    " bigquery.SchemaField('imu_chest_27','STRING'), bigquery.SchemaField('imu_chest_28','STRING'),\n",
    " bigquery.SchemaField('imu_chest_29','STRING'), bigquery.SchemaField('imu_chest_30','STRING'),\n",
    " bigquery.SchemaField('imu_chest_31','STRING'), bigquery.SchemaField('imu_chest_32','STRING'),\n",
    " bigquery.SchemaField('imu_chest_33','STRING'), bigquery.SchemaField('imu_chest_34','STRING'),\n",
    " bigquery.SchemaField('imu_chest_35','STRING'), bigquery.SchemaField('imu_chest_36','STRING'),\n",
    " bigquery.SchemaField('imu_chest_37','STRING'), bigquery.SchemaField('imu_ankle_38','STRING'),\n",
    " bigquery.SchemaField('imu_ankle_39','STRING'), bigquery.SchemaField('imu_ankle_40','STRING'),\n",
    " bigquery.SchemaField('imu_ankle_41','STRING'), bigquery.SchemaField('imu_ankle_42','STRING'),\n",
    " bigquery.SchemaField('imu_ankle_43','STRING'), bigquery.SchemaField('imu_ankle_44','STRING'),\n",
    " bigquery.SchemaField('imu_ankle_45','STRING'), bigquery.SchemaField('imu_ankle_46','STRING'),\n",
    " bigquery.SchemaField('imu_ankle_47','STRING'), bigquery.SchemaField('imu_ankle_48','STRING'),\n",
    " bigquery.SchemaField('imu_ankle_49','STRING'), bigquery.SchemaField('imu_ankle_50','STRING'),\n",
    " bigquery.SchemaField('imu_ankle_51','STRING'), bigquery.SchemaField('imu_ankle_52','STRING'),\n",
    " bigquery.SchemaField('imu_ankle_53','STRING'), bigquery.SchemaField('imu_ankle_54','STRING')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uu4csmiGXjLl"
   },
   "outputs": [],
   "source": [
    "def create_bq_table(table_id: str, project: str, dataset: str, schema: str):\n",
    "    # construct a BigQuery client object\n",
    "    client = bigquery.Client()\n",
    "    table = bigquery.Table('.'.join((project, dataset, table_id)), schema=schema)\n",
    "    table = client.create_table(table)  # Make an API request.\n",
    "    print(\n",
    "        'Created table {}.{}.{}'.format(project, dataset, table_id)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EdTRSVhXjLn",
    "outputId": "27842621-52d0-48f1-89b8-19a151e692c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table qualified-root-293322.PAMAP.subject106\n"
     ]
    }
   ],
   "source": [
    "# create BQ table\n",
    "create_bq_table(table_name, project, dataset, df_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTqZUHOqXjLp"
   },
   "outputs": [],
   "source": [
    "def dataflow_runner(bq_table: str, data_source: str):    \n",
    "    # imports\n",
    "    import re\n",
    "    import csv\n",
    "    import json\n",
    "    import base64\n",
    "    import logging\n",
    "    import argparse\n",
    "    import apache_beam as beam\n",
    "    from apache_beam.options.pipeline_options import PipelineOptions\n",
    "    from apache_beam.io.gcp.bigquery import BigQueryDisposition\n",
    "\n",
    "    class DataIngestion:\n",
    "    # this method parses the input csv and converts into a BigQuery-savable dictionary\n",
    "        def parse_method(self, string_input):\n",
    "            values = re.split(' ', re.sub('\\r\\n', '', re.sub(u'\"', '', string_input)))\n",
    "            row = dict(\n",
    "                zip(('timestamp', 'activity_id', 'heart_rate', 'imu_hand_4', 'imu_hand_5', 'imu_hand_6', 'imu_hand_7', 'imu_hand_8', 'imu_hand_9',\n",
    "                     'imu_hand_10', 'imu_hand_11', 'imu_hand_12', 'imu_hand_13', 'imu_hand_14', 'imu_hand_15', 'imu_hand_16', 'imu_hand_17',\n",
    "                     'imu_hand_18', 'imu_hand_19', 'imu_hand_20', 'imu_chest_21', 'imu_chest_22', 'imu_chest_23', 'imu_chest_24', 'imu_chest_25',\n",
    "                     'imu_chest_26', 'imu_chest_27', 'imu_chest_28', 'imu_chest_29', 'imu_chest_30', 'imu_chest_31', 'imu_chest_32', 'imu_chest_33',\n",
    "                     'imu_chest_34', 'imu_chest_35', 'imu_chest_36', 'imu_chest_37', 'imu_ankle_38', 'imu_ankle_39', 'imu_ankle_40', 'imu_ankle_41',\n",
    "                     'imu_ankle_42', 'imu_ankle_43', 'imu_ankle_44', 'imu_ankle_45', 'imu_ankle_46', 'imu_ankle_47', 'imu_ankle_48', 'imu_ankle_49',\n",
    "                     'imu_ankle_50', 'imu_ankle_51', 'imu_ankle_52', 'imu_ankle_53', 'imu_ankle_54'),\n",
    "                    values))\n",
    "            return row\n",
    "\n",
    "    def run(argv=None, save_main_session=True):\n",
    "        \"\"\"Main entry point; defines and runs the pipeline.\"\"\"\n",
    "\n",
    "        # parameters\n",
    "        table = bq_table\n",
    "        dataset='PAMAP'\n",
    "        project='qualified-root-293322'\n",
    "        source = data_source\n",
    "\n",
    "        options = {\n",
    "            'temp_location': 'gs://kbisong-blessings/temp',\n",
    "            'job_name': 'kbisong-big-data',\n",
    "            'project': 'qualified-root-293322',\n",
    "            'region': 'us-central1',\n",
    "            'max_num_workers': 24,\n",
    "            'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "            'no_save_main_session': True,\n",
    "            'runner': 'DataflowRunner'\n",
    "        }\n",
    "\n",
    "        data_ingestion = DataIngestion()\n",
    "\n",
    "        pipeline_options = PipelineOptions(flags=[], **options)\n",
    "        with beam.Pipeline(options=pipeline_options) as p:\n",
    "            lines = (\n",
    "                p\n",
    "                | 'ReadFile' >> beam.io.ReadFromText(source)\n",
    "                | 'String To BigQuery Row' >> beam.Map(lambda s: data_ingestion.parse_method(s))\n",
    "            )\n",
    "            result = (\n",
    "                lines\n",
    "                | 'WriteBiqQuery' >> beam.io.WriteToBigQuery(table=table, dataset=dataset, project=project,\n",
    "                                                            create_disposition=BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                                                            write_disposition=BigQueryDisposition.WRITE_APPEND,\n",
    "                                                            schema='SCHEMA_AUTODETECT')\n",
    "            )\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        logging.getLogger().setLevel(logging.INFO)\n",
    "        run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MR8lJW2XjLr"
   },
   "outputs": [],
   "source": [
    "optional_source = ['gs://kbisong-blessings/PAMAP2_Dataset/Optional/subject106.dat',\n",
    "         'gs://kbisong-blessings/PAMAP2_Dataset/Optional/subject101.dat',\n",
    "         'gs://kbisong-blessings/PAMAP2_Dataset/Optional/subject109.dat',\n",
    "         'gs://kbisong-blessings/PAMAP2_Dataset/Optional/subject108.dat',\n",
    "         'gs://kbisong-blessings/PAMAP2_Dataset/Optional/subject105.dat']\n",
    "\n",
    "protocol_source = ['gs://kbisong-blessings/PAMAP2_Dataset/Protocol/subject101.dat',\n",
    "                  'gs://kbisong-blessings/PAMAP2_Dataset/Protocol/subject102.dat',\n",
    "                  'gs://kbisong-blessings/PAMAP2_Dataset/Protocol/subject103.dat',\n",
    "                  'gs://kbisong-blessings/PAMAP2_Dataset/Protocol/subject104.dat',\n",
    "                  'gs://kbisong-blessings/PAMAP2_Dataset/Protocol/subject105.dat',\n",
    "                  'gs://kbisong-blessings/PAMAP2_Dataset/Protocol/subject106.dat',\n",
    "                  'gs://kbisong-blessings/PAMAP2_Dataset/Protocol/subject107.dat',\n",
    "                  'gs://kbisong-blessings/PAMAP2_Dataset/Protocol/subject108.dat',\n",
    "                  'gs://kbisong-blessings/PAMAP2_Dataset/Protocol/subject109.dat']\n",
    "\n",
    "table = {'Optional':  optional_source, 'Protocol': protocol_source}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THPStD4SXjLu",
    "outputId": "304effc8-4943-433a-bdce-8bd1ad2de44b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp6mvmtkpb', 'apache-beam==2.24.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp6mvmtkpb', 'apache-beam==2.24.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "INFO:root:Using Python SDK docker image: apache/beam_python3.7_sdk:2.24.0. If the image is not available at local, we will try to pull from hub.docker.com\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Defaulting to the temp_location as staging_location: gs://kbisong-blessings/temp\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603481111.433509/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603481111.433509/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603481111.433509/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603481111.433509/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603481111.433509/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603481111.433509/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl in 0 seconds.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-10-23T19:25:14.400449Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-10-23_12_25_13-14252547739810459672'\n",
      " location: 'us-central1'\n",
      " name: 'kbisong-big-data'\n",
      " projectId: 'qualified-root-293322'\n",
      " stageStates: []\n",
      " startTime: '2020-10-23T19:25:14.400449Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-10-23_12_25_13-14252547739810459672]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2020-10-23_12_25_13-14252547739810459672\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-central1/2020-10-23_12_25_13-14252547739810459672?project=qualified-root-293322\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_12_25_13-14252547739810459672 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:13.505Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-10-23_12_25_13-14252547739810459672.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:13.505Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-10-23_12_25_13-14252547739810459672. The number of workers will be between 1 and 24.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:17.045Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-b.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:17.681Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:17.730Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:17.771Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:17.825Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:17.856Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:17.885Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.045Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.098Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.144Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17 for input s11.WrittenFiles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.189Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion, into producer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.229Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.266Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.301Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles) into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.340Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.374Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.419Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17-u32 for input s18.None-c30\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.455Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion/Unzipped-1, into producer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.491Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround into WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.522Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify into WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.559Z: JOB_MESSAGE_DETAILED: Fusing consumer String To BigQuery Row into ReadFile/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.589Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal into String To BigQuery Row\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.619Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination into WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.657Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile) into WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.694Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.727Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.762Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.804Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.852Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.890Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile into WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.946Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:18.990Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.052Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.077Z: JOB_MESSAGE_DETAILED: Fusing siblings WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs and WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.112Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs) into WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.157Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables into WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.195Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.228Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.262Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.297Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.326Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.368Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.410Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.439Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.472Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.502Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.694Z: JOB_MESSAGE_DEBUG: Executing wait step start46\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.776Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_12_25_13-14252547739810459672 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.807Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.832Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.875Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-b...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.888Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.928Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.943Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.962Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.964Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:19.969Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:20.016Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:20.037Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:20.060Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:20.095Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:25:20.147Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:26:41.420Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:28:23.062Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:28:23.100Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:29:17.736Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:29:17.792Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:29:17.830Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:29:17.869Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:29:17.900Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:29:17.931Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:29:17.970Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:29:17.999Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:29:18.020Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:29:18.035Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:29:18.036Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:29:18.059Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:29:18.075Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:29:18.086Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:29:18.109Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:29:18.134Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:29:18.148Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:29:18.171Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:29:18.197Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:29:18.273Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:29:18.348Z: JOB_MESSAGE_BASIC: Executing operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:01.709Z: JOB_MESSAGE_BASIC: Finished operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:01.798Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:01.861Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:01.936Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:04.973Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:05.082Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:05.133Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:05.213Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:17.299Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:17.371Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:17.407Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).TemporaryTables\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:17.456Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:17.502Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:17.543Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:17.563Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:17.579Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:17.599Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:17.614Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:17.644Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:17.667Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:17.700Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:17.712Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:17.746Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:17.785Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/Flatten.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:17.842Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:30.454Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:30.547Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:30.626Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:30.672Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:30.747Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:30.838Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:31.656Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:31.740Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:31.795Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:31.897Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:33.784Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:33.859Z: JOB_MESSAGE_DEBUG: Executing success step success44\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:33.932Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:33.989Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:30:34.063Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:22.961Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:23.015Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:23.074Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_12_25_13-14252547739810459672 is in state JOB_STATE_DONE\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp_z2kltm3', 'apache-beam==2.24.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp_z2kltm3', 'apache-beam==2.24.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "INFO:root:Using Python SDK docker image: apache/beam_python3.7_sdk:2.24.0. If the image is not available at local, we will try to pull from hub.docker.com\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Defaulting to the temp_location as staging_location: gs://kbisong-blessings/temp\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603481504.923635/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603481504.923635/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603481504.923635/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603481504.923635/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603481504.923635/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603481504.923635/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl in 0 seconds.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-10-23T19:31:48.204215Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-10-23_12_31_47-15185282241985010698'\n",
      " location: 'us-central1'\n",
      " name: 'kbisong-big-data'\n",
      " projectId: 'qualified-root-293322'\n",
      " stageStates: []\n",
      " startTime: '2020-10-23T19:31:48.204215Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-10-23_12_31_47-15185282241985010698]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2020-10-23_12_31_47-15185282241985010698\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-central1/2020-10-23_12_31_47-15185282241985010698?project=qualified-root-293322\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_12_31_47-15185282241985010698 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:47.165Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-10-23_12_31_47-15185282241985010698. The number of workers will be between 1 and 24.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:47.165Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-10-23_12_31_47-15185282241985010698.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:51.118Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-b.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:51.841Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:51.882Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:51.914Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:51.953Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:51.998Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:52.043Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:52.204Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:52.255Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:52.309Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17 for input s11.WrittenFiles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:52.344Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion, into producer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:52.377Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:52.412Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:52.448Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles) into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:52.484Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:52.515Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:52.550Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17-u32 for input s18.None-c30\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:52.587Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion/Unzipped-1, into producer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:52.617Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround into WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:52.644Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify into WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:52.675Z: JOB_MESSAGE_DETAILED: Fusing consumer String To BigQuery Row into ReadFile/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:52.721Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal into String To BigQuery Row\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:52.754Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination into WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:52.788Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile) into WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:52.840Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:52.876Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:52.911Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:52.946Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:52.983Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:53.151Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile into WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:53.193Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:53.220Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:53.258Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:53.307Z: JOB_MESSAGE_DETAILED: Fusing siblings WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs and WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:53.347Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs) into WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:53.378Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables into WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:53.416Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:53.454Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:53.488Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:53.523Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:53.561Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:53.587Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:53.622Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:53.658Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:53.695Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:53.733Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:54.033Z: JOB_MESSAGE_DEBUG: Executing wait step start46\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:54.102Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:54.142Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:54.142Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:54.177Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:54.185Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-b...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:54.209Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:54.234Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:54.234Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:54.234Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:54.237Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:54.291Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:54.324Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:54.361Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:54.394Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:31:54.427Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_12_31_47-15185282241985010698 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:33:04.303Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:34:48.517Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:34:48.549Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:35:40.202Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:35:40.287Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:35:40.325Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:35:40.369Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:35:40.408Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:35:40.440Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:35:40.483Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:35:40.511Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:35:40.516Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:35:40.536Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:35:40.555Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:35:40.581Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:35:40.590Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:35:40.620Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:35:40.629Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:35:40.643Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:35:40.663Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:35:40.690Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:35:40.713Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:35:40.753Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:35:40.825Z: JOB_MESSAGE_BASIC: Executing operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:36:47.804Z: JOB_MESSAGE_BASIC: Finished operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:36:47.888Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:36:47.948Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:36:48.013Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:36:51.103Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:36:51.166Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:36:51.219Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:36:51.298Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:00.733Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:00.805Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:00.840Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).TemporaryTables\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:00.874Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:00.924Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:00.986Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:01.024Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:01.037Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:01.043Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:01.052Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:01.089Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:01.104Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:01.140Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:01.143Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:01.180Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:01.204Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/Flatten.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:01.246Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:23.943Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:24.023Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:24.083Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:24.140Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:24.212Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:24.277Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:27.002Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:27.065Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:27.116Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:27.195Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:29.378Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:29.452Z: JOB_MESSAGE_DEBUG: Executing success step success44\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:29.524Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:29.578Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:37:29.607Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:25.370Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:25.427Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:25.457Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_12_31_47-15185282241985010698 is in state JOB_STATE_DONE\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpn5oaq738', 'apache-beam==2.24.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpn5oaq738', 'apache-beam==2.24.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "INFO:root:Using Python SDK docker image: apache/beam_python3.7_sdk:2.24.0. If the image is not available at local, we will try to pull from hub.docker.com\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Defaulting to the temp_location as staging_location: gs://kbisong-blessings/temp\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603481928.534113/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603481928.534113/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603481928.534113/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603481928.534113/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603481928.534113/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603481928.534113/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl in 0 seconds.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-10-23T19:38:52.397260Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-10-23_12_38_51-16854346224246740087'\n",
      " location: 'us-central1'\n",
      " name: 'kbisong-big-data'\n",
      " projectId: 'qualified-root-293322'\n",
      " stageStates: []\n",
      " startTime: '2020-10-23T19:38:52.397260Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-10-23_12_38_51-16854346224246740087]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2020-10-23_12_38_51-16854346224246740087\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-central1/2020-10-23_12_38_51-16854346224246740087?project=qualified-root-293322\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_12_38_51-16854346224246740087 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:51.186Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-10-23_12_38_51-16854346224246740087.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:51.186Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-10-23_12_38_51-16854346224246740087. The number of workers will be between 1 and 24.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:55.132Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-b.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:55.808Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:55.844Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:55.876Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:55.912Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:55.958Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.004Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.171Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.222Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.259Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17 for input s11.WrittenFiles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.290Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion, into producer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.332Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.371Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.441Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles) into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.466Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.504Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.541Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17-u32 for input s18.None-c30\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.569Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion/Unzipped-1, into producer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.594Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround into WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.627Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify into WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.660Z: JOB_MESSAGE_DETAILED: Fusing consumer String To BigQuery Row into ReadFile/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.695Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal into String To BigQuery Row\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.735Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination into WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.766Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile) into WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.803Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.834Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.857Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.894Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.920Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.957Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile into WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:56.993Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:57.048Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:57.074Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:57.128Z: JOB_MESSAGE_DETAILED: Fusing siblings WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs and WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:57.163Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs) into WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:57.193Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables into WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:57.224Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:57.252Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:57.277Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:57.301Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:57.338Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:57.368Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:57.443Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:57.475Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:57.512Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:57.542Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_12_38_51-16854346224246740087 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:57.983Z: JOB_MESSAGE_DEBUG: Executing wait step start46\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:58.141Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:58.175Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:58.186Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:58.203Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:58.224Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-b...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:58.237Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:58.266Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:58.268Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:58.283Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:58.283Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:58.320Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:58.348Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:58.381Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:58.444Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:38:58.490Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:39:25.888Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:41:16.799Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:41:16.836Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:42:09.224Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:42:09.306Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:42:09.346Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:42:09.383Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:42:09.410Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:42:09.446Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:42:09.503Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:42:09.526Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:42:09.542Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:42:09.563Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:42:09.565Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:42:09.587Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:42:09.606Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:42:09.618Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:42:09.642Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:42:09.664Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:42:09.667Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:42:09.688Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:42:09.711Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:42:09.749Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:42:09.812Z: JOB_MESSAGE_BASIC: Executing operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:03.792Z: JOB_MESSAGE_BASIC: Finished operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:03.848Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:03.892Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:03.950Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:07.084Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:07.159Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:07.196Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:07.258Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:17.441Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:17.511Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:17.537Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).TemporaryTables\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:17.560Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:17.589Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:17.624Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:17.637Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:17.679Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:17.689Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:17.703Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:17.723Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:17.727Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:17.749Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:17.776Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:17.780Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:17.822Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/Flatten.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:17.852Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:42.486Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:42.564Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:42.618Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:42.669Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:42.728Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:42.780Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:44.588Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:44.643Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:44.681Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:44.742Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:46.905Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:46.971Z: JOB_MESSAGE_DEBUG: Executing success step success44\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:47.047Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:47.105Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:43:47.148Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:44:42.890Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:44:42.930Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:44:42.956Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_12_38_51-16854346224246740087 is in state JOB_STATE_DONE\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpkij614f0', 'apache-beam==2.24.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpkij614f0', 'apache-beam==2.24.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "INFO:root:Using Python SDK docker image: apache/beam_python3.7_sdk:2.24.0. If the image is not available at local, we will try to pull from hub.docker.com\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Defaulting to the temp_location as staging_location: gs://kbisong-blessings/temp\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603482307.866051/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603482307.866051/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603482307.866051/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603482307.866051/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603482307.866051/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603482307.866051/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl in 0 seconds.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-10-23T19:45:10.985534Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-10-23_12_45_09-4863726480414397351'\n",
      " location: 'us-central1'\n",
      " name: 'kbisong-big-data'\n",
      " projectId: 'qualified-root-293322'\n",
      " stageStates: []\n",
      " startTime: '2020-10-23T19:45:10.985534Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-10-23_12_45_09-4863726480414397351]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2020-10-23_12_45_09-4863726480414397351\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-central1/2020-10-23_12_45_09-4863726480414397351?project=qualified-root-293322\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_12_45_09-4863726480414397351 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:10.009Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-10-23_12_45_09-4863726480414397351.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:10.009Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-10-23_12_45_09-4863726480414397351. The number of workers will be between 1 and 24.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:16.037Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-b.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:16.781Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:16.818Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:16.852Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:16.879Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:16.917Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:16.938Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.058Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.114Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.144Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17 for input s11.WrittenFiles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.185Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion, into producer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.231Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.262Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.340Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles) into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.382Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.413Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.447Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17-u32 for input s18.None-c30\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.481Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion/Unzipped-1, into producer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.510Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround into WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.546Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify into WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.580Z: JOB_MESSAGE_DETAILED: Fusing consumer String To BigQuery Row into ReadFile/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.609Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal into String To BigQuery Row\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.647Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination into WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.686Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile) into WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.723Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.758Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.793Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.818Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.840Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.873Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile into WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.909Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.945Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:17.980Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.015Z: JOB_MESSAGE_DETAILED: Fusing siblings WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs and WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.040Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs) into WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.069Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables into WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.102Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.136Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.165Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.196Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.236Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.265Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.301Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.336Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.373Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.399Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.673Z: JOB_MESSAGE_DEBUG: Executing wait step start46\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.750Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.778Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.789Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.811Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.822Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-b...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.834Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.863Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.865Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.872Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.873Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.916Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.933Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.961Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:18.994Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:19.027Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_12_45_09-4863726480414397351 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:45:46.515Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:47:37.882Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:47:37.920Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:48:33.367Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:48:33.433Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:48:33.467Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:48:33.492Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:48:33.527Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:48:33.565Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:48:33.597Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:48:33.619Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:48:33.620Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:48:33.647Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:48:33.660Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:48:33.667Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:48:33.680Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:48:33.705Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:48:33.717Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:48:33.736Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:48:33.743Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:48:33.775Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:48:33.800Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:48:33.834Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:48:33.905Z: JOB_MESSAGE_BASIC: Executing operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:29.794Z: JOB_MESSAGE_BASIC: Finished operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:29.860Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:29.907Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:29.975Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:33.306Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:33.382Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:33.435Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:33.523Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:39.158Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:39.230Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:39.267Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).TemporaryTables\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:39.304Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:39.338Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:39.364Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:39.390Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:39.391Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:39.422Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:39.423Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:39.451Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:39.493Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:39.522Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:39.561Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:39.597Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:39.638Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/Flatten.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:49:39.686Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:50:14.540Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:50:14.625Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:50:14.700Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:50:14.788Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:50:14.873Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:50:14.956Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:50:16.735Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:50:16.802Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:50:16.854Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:50:16.930Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:50:17.863Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:50:17.941Z: JOB_MESSAGE_DEBUG: Executing success step success44\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:50:18.044Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:50:18.100Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:50:18.131Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:18.485Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:18.529Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:18.562Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_12_45_09-4863726480414397351 is in state JOB_STATE_DONE\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpospzzqvr', 'apache-beam==2.24.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpospzzqvr', 'apache-beam==2.24.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "INFO:root:Using Python SDK docker image: apache/beam_python3.7_sdk:2.24.0. If the image is not available at local, we will try to pull from hub.docker.com\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Defaulting to the temp_location as staging_location: gs://kbisong-blessings/temp\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603482701.298670/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603482701.298670/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603482701.298670/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603482701.298670/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603482701.298670/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603482701.298670/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl in 0 seconds.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-10-23T19:51:44.393276Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-10-23_12_51_43-6529523570624454751'\n",
      " location: 'us-central1'\n",
      " name: 'kbisong-big-data'\n",
      " projectId: 'qualified-root-293322'\n",
      " stageStates: []\n",
      " startTime: '2020-10-23T19:51:44.393276Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-10-23_12_51_43-6529523570624454751]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2020-10-23_12_51_43-6529523570624454751\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-central1/2020-10-23_12_51_43-6529523570624454751?project=qualified-root-293322\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_12_51_43-6529523570624454751 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:43.495Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-10-23_12_51_43-6529523570624454751.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:43.495Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-10-23_12_51_43-6529523570624454751. The number of workers will be between 1 and 24.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:47.341Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-b.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:47.927Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:47.962Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:48.005Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:48.041Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:48.099Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:48.129Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:48.287Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:48.353Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:48.399Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17 for input s11.WrittenFiles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:48.432Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion, into producer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:48.471Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:48.495Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:48.611Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles) into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:48.669Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:48.705Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:48.730Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17-u32 for input s18.None-c30\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:48.762Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion/Unzipped-1, into producer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:48.795Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround into WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:48.833Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify into WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:48.861Z: JOB_MESSAGE_DETAILED: Fusing consumer String To BigQuery Row into ReadFile/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:48.891Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal into String To BigQuery Row\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:48.922Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination into WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:48.953Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile) into WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:48.987Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:49.021Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:49.056Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:49.100Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:49.128Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:49.164Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile into WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:49.200Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:49.227Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:49.264Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:49.299Z: JOB_MESSAGE_DETAILED: Fusing siblings WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs and WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:49.335Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs) into WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:49.383Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables into WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:49.418Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:49.451Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:49.486Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:49.516Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:49.550Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:49.577Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:49.608Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:49.670Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:49.698Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:49.725Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_12_51_43-6529523570624454751 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:49.911Z: JOB_MESSAGE_DEBUG: Executing wait step start46\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:50.002Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:50.031Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:50.057Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:50.069Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:50.089Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-b...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:50.100Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:50.125Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:50.137Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:50.154Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:50.154Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:50.185Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:50.216Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:50.252Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:50.289Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:51:50.325Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:52:18.092Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:54:00.854Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:54:00.894Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:54:55.813Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:54:55.890Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:54:55.911Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:54:55.935Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:54:55.961Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:54:55.979Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:54:56.003Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:54:56.028Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:54:56.037Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:54:56.047Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:54:56.058Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:54:56.083Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:54:56.086Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:54:56.113Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:54:56.115Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:54:56.141Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:54:56.145Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:54:56.170Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:54:56.199Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:54:56.221Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:54:56.284Z: JOB_MESSAGE_BASIC: Executing operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:55:45.912Z: JOB_MESSAGE_BASIC: Finished operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:55:45.997Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:55:46.184Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:55:46.252Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:55:49.294Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:55:49.378Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:55:49.437Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:55:49.515Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:02.205Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:02.272Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:02.310Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).TemporaryTables\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:02.341Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:02.374Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:02.410Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:02.449Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:02.452Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:02.493Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:02.505Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:02.515Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:02.522Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:02.564Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:02.585Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:02.617Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:02.654Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/Flatten.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:02.682Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:25.280Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:25.374Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:25.459Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:25.521Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:25.613Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:25.699Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:27.682Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:27.753Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:27.801Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:27.867Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:29.045Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:29.165Z: JOB_MESSAGE_DEBUG: Executing success step success44\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:29.233Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:29.289Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:56:29.326Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:21.333Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:21.402Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:21.451Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_12_51_43-6529523570624454751 is in state JOB_STATE_DONE\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpzi_pmrox', 'apache-beam==2.24.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpzi_pmrox', 'apache-beam==2.24.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "INFO:root:Using Python SDK docker image: apache/beam_python3.7_sdk:2.24.0. If the image is not available at local, we will try to pull from hub.docker.com\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Defaulting to the temp_location as staging_location: gs://kbisong-blessings/temp\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603483059.694613/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603483059.694613/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603483059.694613/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603483059.694613/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603483059.694613/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603483059.694613/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl in 0 seconds.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-10-23T19:57:43.208738Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-10-23_12_57_42-17072568941885478055'\n",
      " location: 'us-central1'\n",
      " name: 'kbisong-big-data'\n",
      " projectId: 'qualified-root-293322'\n",
      " stageStates: []\n",
      " startTime: '2020-10-23T19:57:43.208738Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-10-23_12_57_42-17072568941885478055]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2020-10-23_12_57_42-17072568941885478055\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-central1/2020-10-23_12_57_42-17072568941885478055?project=qualified-root-293322\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_12_57_42-17072568941885478055 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:42.191Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-10-23_12_57_42-17072568941885478055. The number of workers will be between 1 and 24.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:42.191Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-10-23_12_57_42-17072568941885478055.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:46.371Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-b.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:47.116Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:47.336Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:47.375Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:47.408Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:47.453Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:47.489Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:47.636Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:47.688Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:47.716Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17 for input s11.WrittenFiles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:47.747Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion, into producer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:47.768Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:47.798Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:47.830Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles) into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:47.859Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:47.887Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:47.923Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17-u32 for input s18.None-c30\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:47.965Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion/Unzipped-1, into producer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:47.998Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround into WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.037Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify into WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.134Z: JOB_MESSAGE_DETAILED: Fusing consumer String To BigQuery Row into ReadFile/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.174Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal into String To BigQuery Row\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.204Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination into WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.244Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile) into WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.278Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.307Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.338Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.393Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.430Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.463Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile into WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.488Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.515Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.549Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.587Z: JOB_MESSAGE_DETAILED: Fusing siblings WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs and WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.622Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs) into WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.655Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables into WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.697Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.721Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.754Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.793Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.833Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.870Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.898Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.931Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.969Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:48.995Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:49.196Z: JOB_MESSAGE_DEBUG: Executing wait step start46\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:49.276Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:49.302Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:49.314Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:49.335Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:49.344Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-b...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:49.369Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:49.396Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:49.399Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:49.413Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:49.413Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:49.445Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:49.477Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:49.509Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:49.550Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:57:49.600Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_12_57_42-17072568941885478055 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T19:58:17.769Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:00:04.689Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:00:04.721Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:00:59.774Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:00:59.846Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:00:59.877Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:00:59.913Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:00:59.951Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:00:59.987Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:01:00.039Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:01:00.045Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:01:00.077Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:01:00.107Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:01:00.121Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:01:00.127Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:01:00.132Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:01:00.162Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:01:00.172Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:01:00.185Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:01:00.196Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:01:00.230Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:01:00.266Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:01:00.298Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:01:00.369Z: JOB_MESSAGE_BASIC: Executing operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:17.309Z: JOB_MESSAGE_BASIC: Finished operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:17.374Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:17.425Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:17.496Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:20.536Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:20.600Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:20.653Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:20.715Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:28.418Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:28.490Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:28.523Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).TemporaryTables\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:28.556Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:28.577Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:28.611Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:28.637Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:28.657Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:28.678Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:28.686Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:28.703Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:28.721Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:28.756Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:28.763Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:28.788Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:28.834Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/Flatten.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:02:28.858Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:03:02.618Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:03:02.716Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:03:02.788Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:03:02.840Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:03:02.908Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:03:02.989Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:03:05.861Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:03:05.938Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:03:05.991Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:03:06.077Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:03:07.986Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:03:08.082Z: JOB_MESSAGE_DEBUG: Executing success step success44\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:03:08.167Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:03:08.215Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:03:08.238Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:03.199Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:03.243Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:03.280Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_12_57_42-17072568941885478055 is in state JOB_STATE_DONE\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpcunhst6q', 'apache-beam==2.24.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpcunhst6q', 'apache-beam==2.24.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "INFO:root:Using Python SDK docker image: apache/beam_python3.7_sdk:2.24.0. If the image is not available at local, we will try to pull from hub.docker.com\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Defaulting to the temp_location as staging_location: gs://kbisong-blessings/temp\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603483463.603182/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603483463.603182/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603483463.603182/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603483463.603182/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603483463.603182/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603483463.603182/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl in 0 seconds.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-10-23T20:04:27.055182Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-10-23_13_04_25-13628818389559024756'\n",
      " location: 'us-central1'\n",
      " name: 'kbisong-big-data'\n",
      " projectId: 'qualified-root-293322'\n",
      " stageStates: []\n",
      " startTime: '2020-10-23T20:04:27.055182Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-10-23_13_04_25-13628818389559024756]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2020-10-23_13_04_25-13628818389559024756\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-central1/2020-10-23_13_04_25-13628818389559024756?project=qualified-root-293322\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_13_04_25-13628818389559024756 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:25.999Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-10-23_13_04_25-13628818389559024756. The number of workers will be between 1 and 24.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:25.999Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-10-23_13_04_25-13628818389559024756.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:30.035Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-b.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:30.774Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:30.810Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:30.863Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:30.906Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:30.951Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:30.988Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:31.141Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:31.216Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:31.257Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17 for input s11.WrittenFiles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:31.292Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion, into producer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:31.324Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:31.355Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:31.385Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles) into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:31.418Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:31.467Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:31.535Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17-u32 for input s18.None-c30\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:31.568Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion/Unzipped-1, into producer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:31.605Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround into WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:31.638Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify into WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:31.677Z: JOB_MESSAGE_DETAILED: Fusing consumer String To BigQuery Row into ReadFile/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:31.712Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal into String To BigQuery Row\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:31.745Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination into WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:31.787Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile) into WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:31.818Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:31.863Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:31.895Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:31.933Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:31.971Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.013Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile into WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.050Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.076Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.114Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.159Z: JOB_MESSAGE_DETAILED: Fusing siblings WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs and WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.196Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs) into WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.232Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables into WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.266Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.302Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.334Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.370Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.409Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.446Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.491Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_13_04_25-13628818389559024756 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.539Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.572Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.609Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.792Z: JOB_MESSAGE_DEBUG: Executing wait step start46\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.877Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.909Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.921Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.935Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.946Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-b...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.966Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.996Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:32.998Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:33.008Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:33.008Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:33.050Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:33.068Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:33.105Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:33.140Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:04:33.171Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:05:05.755Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n",
      "INFO:oauth2client.transport:Refreshing due to a 401 (attempt 1/2)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:07:47.188Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:07:47.305Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:07:47.333Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:07:47.372Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:07:47.404Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:07:47.441Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:07:47.479Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:07:47.507Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:07:47.514Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:07:47.537Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:07:47.549Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:07:47.571Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:07:47.584Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:07:47.605Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:07:47.628Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:07:47.634Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:07:47.663Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:07:47.703Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:07:47.747Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:07:47.784Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:07:47.858Z: JOB_MESSAGE_BASIC: Executing operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:14.779Z: JOB_MESSAGE_BASIC: Finished operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:14.854Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:14.914Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:14.988Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:17.132Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:17.207Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:17.253Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:17.326Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:29.026Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:29.105Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:29.147Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).TemporaryTables\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:29.193Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:29.233Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:29.275Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:29.291Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:29.312Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:29.331Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:29.338Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:29.354Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:29.371Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:29.394Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:29.417Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:29.429Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:29.493Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:09:29.517Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/Flatten.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:10:14.387Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:10:14.483Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:10:14.559Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:10:14.613Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:10:14.686Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:10:14.761Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:10:16.554Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:10:16.642Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:10:16.697Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:10:16.765Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:10:17.941Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:10:18.007Z: JOB_MESSAGE_DEBUG: Executing success step success44\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:10:18.091Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:10:18.159Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:10:18.198Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:11:00.603Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:11:00.647Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:11:00.693Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_13_04_25-13628818389559024756 is in state JOB_STATE_DONE\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpkg_stnvb', 'apache-beam==2.24.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpkg_stnvb', 'apache-beam==2.24.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "INFO:root:Using Python SDK docker image: apache/beam_python3.7_sdk:2.24.0. If the image is not available at local, we will try to pull from hub.docker.com\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Defaulting to the temp_location as staging_location: gs://kbisong-blessings/temp\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603483882.765193/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603483882.765193/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603483882.765193/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603483882.765193/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603483882.765193/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603483882.765193/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl in 0 seconds.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-10-23T20:11:26.203096Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-10-23_13_11_25-18333081337501847837'\n",
      " location: 'us-central1'\n",
      " name: 'kbisong-big-data'\n",
      " projectId: 'qualified-root-293322'\n",
      " stageStates: []\n",
      " startTime: '2020-10-23T20:11:26.203096Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-10-23_13_11_25-18333081337501847837]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2020-10-23_13_11_25-18333081337501847837\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-central1/2020-10-23_13_11_25-18333081337501847837?project=qualified-root-293322\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:11:59.113Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:13:43.240Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:13:43.269Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:14:35.251Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:14:35.325Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:14:35.353Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:14:35.382Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:14:35.423Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:14:35.466Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:14:35.499Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:14:35.520Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:14:35.533Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:14:35.553Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:14:35.567Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:14:35.591Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:14:35.594Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:14:35.625Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:14:35.636Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:14:35.642Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:14:35.652Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:14:35.682Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:14:35.707Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:14:35.742Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:14:35.800Z: JOB_MESSAGE_BASIC: Executing operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:15:49.769Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:15:49.840Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:15:49.873Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).TemporaryTables\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:15:49.898Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:15:49.920Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:15:49.952Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:15:49.968Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:15:49.988Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:15:50.014Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:15:50.026Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:15:50.045Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:15:50.058Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:15:50.090Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:15:50.096Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:15:50.123Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:15:50.158Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/Flatten.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:15:50.217Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:16:13.682Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:16:13.744Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:16:13.816Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:16:13.856Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:16:13.940Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:16:14.035Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:16:14.708Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:16:14.774Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:16:14.816Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:16:14.880Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:16:16.055Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:16:16.112Z: JOB_MESSAGE_DEBUG: Executing success step success44\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:16:16.180Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:16:16.230Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:16:16.267Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:02.717Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:02.752Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:02.781Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_13_11_25-18333081337501847837 is in state JOB_STATE_DONE\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpdywpowpi', 'apache-beam==2.24.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpdywpowpi', 'apache-beam==2.24.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "INFO:root:Using Python SDK docker image: apache/beam_python3.7_sdk:2.24.0. If the image is not available at local, we will try to pull from hub.docker.com\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Defaulting to the temp_location as staging_location: gs://kbisong-blessings/temp\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603484246.568977/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603484246.568977/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603484246.568977/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603484246.568977/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603484246.568977/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603484246.568977/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl in 0 seconds.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-10-23T20:17:29.773324Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-10-23_13_17_28-10448254214696742372'\n",
      " location: 'us-central1'\n",
      " name: 'kbisong-big-data'\n",
      " projectId: 'qualified-root-293322'\n",
      " stageStates: []\n",
      " startTime: '2020-10-23T20:17:29.773324Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-10-23_13_17_28-10448254214696742372]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2020-10-23_13_17_28-10448254214696742372\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-central1/2020-10-23_13_17_28-10448254214696742372?project=qualified-root-293322\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_13_17_28-10448254214696742372 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:28.846Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-10-23_13_17_28-10448254214696742372. The number of workers will be between 1 and 24.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:28.847Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-10-23_13_17_28-10448254214696742372.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:33.950Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-b.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:34.785Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:34.814Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:34.841Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:34.865Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:34.907Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:34.989Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.106Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.140Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.165Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17 for input s11.WrittenFiles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.199Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion, into producer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.227Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.251Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.288Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles) into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.319Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.343Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.378Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17-u32 for input s18.None-c30\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.466Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion/Unzipped-1, into producer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.499Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround into WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.533Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify into WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.571Z: JOB_MESSAGE_DETAILED: Fusing consumer String To BigQuery Row into ReadFile/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.607Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal into String To BigQuery Row\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.663Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination into WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.684Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile) into WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.726Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.746Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.782Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.818Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.853Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.879Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile into WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.908Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.939Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:35.978Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36Z: JOB_MESSAGE_DETAILED: Fusing siblings WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs and WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36.036Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs) into WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36.072Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables into WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36.108Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36.132Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36.167Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36.190Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36.212Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36.245Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36.273Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36.308Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36.340Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36.375Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36.618Z: JOB_MESSAGE_DEBUG: Executing wait step start46\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36.691Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36.722Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36.746Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36.747Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36.780Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-b...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36.780Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36.812Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36.815Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36.839Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:36.839Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:37.223Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:37.237Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:37.281Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:37.307Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:17:37.344Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_13_17_28-10448254214696742372 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:18:05.616Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:19:45.873Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:19:45.905Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:20:42.106Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:20:42.172Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:20:42.208Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:20:42.244Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:20:42.268Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:20:42.304Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:20:42.334Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:20:42.356Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:20:42.368Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:20:42.380Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:20:42.407Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:20:42.427Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:20:42.441Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:20:42.451Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:20:42.477Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:20:42.504Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:20:42.511Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:20:42.544Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:20:42.584Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:20:42.629Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:20:42.689Z: JOB_MESSAGE_BASIC: Executing operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:21:52.658Z: JOB_MESSAGE_BASIC: Finished operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:21:52.737Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:21:52.790Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:21:52.880Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:21:55.904Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:21:55.967Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:21:56.010Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:21:56.121Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:04.459Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:04.530Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:04.565Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).TemporaryTables\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:04.601Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:04.634Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:04.672Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:04.690Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:04.707Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:04.736Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:04.740Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:04.761Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:04.776Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:04.803Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:04.810Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:04.847Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:04.879Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/Flatten.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:04.925Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:38.610Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:38.668Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:38.757Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:38.812Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:38.869Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:38.914Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:40.610Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:40.682Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:40.748Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:40.814Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:40.961Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:41.022Z: JOB_MESSAGE_DEBUG: Executing success step success44\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:41.108Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:41.158Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:22:41.187Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:23:34.468Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:23:34.526Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:23:34.554Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_13_17_28-10448254214696742372 is in state JOB_STATE_DONE\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp8ndz2kkr', 'apache-beam==2.24.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp8ndz2kkr', 'apache-beam==2.24.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "INFO:root:Using Python SDK docker image: apache/beam_python3.7_sdk:2.24.0. If the image is not available at local, we will try to pull from hub.docker.com\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Defaulting to the temp_location as staging_location: gs://kbisong-blessings/temp\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603484635.043210/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603484635.043210/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603484635.043210/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603484635.043210/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603484635.043210/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603484635.043210/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl in 0 seconds.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-10-23T20:23:58.312377Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-10-23_13_23_57-6410512273965944224'\n",
      " location: 'us-central1'\n",
      " name: 'kbisong-big-data'\n",
      " projectId: 'qualified-root-293322'\n",
      " stageStates: []\n",
      " startTime: '2020-10-23T20:23:58.312377Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-10-23_13_23_57-6410512273965944224]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2020-10-23_13_23_57-6410512273965944224\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-central1/2020-10-23_13_23_57-6410512273965944224?project=qualified-root-293322\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_13_23_57-6410512273965944224 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:23:57.211Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-10-23_13_23_57-6410512273965944224.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:23:57.211Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-10-23_13_23_57-6410512273965944224. The number of workers will be between 1 and 24.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:00.976Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-b.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:01.552Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:01.586Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:01.607Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:01.634Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:01.678Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:01.731Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:01.862Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:01.914Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:01.939Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17 for input s11.WrittenFiles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:01.971Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion, into producer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.004Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.029Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.057Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles) into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.095Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.128Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.162Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17-u32 for input s18.None-c30\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.205Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion/Unzipped-1, into producer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.249Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround into WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.301Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify into WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.344Z: JOB_MESSAGE_DETAILED: Fusing consumer String To BigQuery Row into ReadFile/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.377Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal into String To BigQuery Row\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.408Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination into WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.440Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile) into WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.468Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.507Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.538Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.574Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.607Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.642Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile into WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.675Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.710Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.741Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.773Z: JOB_MESSAGE_DETAILED: Fusing siblings WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs and WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.805Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs) into WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.837Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables into WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.870Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.909Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.944Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:02.980Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:03.017Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:03.046Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:03.075Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:03.098Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:03.124Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:03.156Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:03.397Z: JOB_MESSAGE_DEBUG: Executing wait step start46\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:03.471Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:03.498Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:03.508Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_13_23_57-6410512273965944224 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:03.530Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:03.539Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-b...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:03.558Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:03.582Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:03.585Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:03.597Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:03.597Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:03.630Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:03.780Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:03.817Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:03.854Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:03.885Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:24:31.397Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:26:13.222Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:26:13.253Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:27:08.348Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:27:08.427Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:27:08.459Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:27:08.492Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:27:08.518Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:27:08.564Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:27:08.596Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:27:08.629Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:27:08.629Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:27:08.649Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:27:08.664Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:27:08.690Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:27:08.699Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:27:08.707Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:27:08.732Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:27:08.752Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:27:08.769Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:27:08.803Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:27:08.837Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:27:08.879Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:27:08.943Z: JOB_MESSAGE_BASIC: Executing operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:25.951Z: JOB_MESSAGE_BASIC: Finished operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:26.012Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:26.069Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:26.144Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:28.194Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:28.257Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:28.310Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:28.383Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:40.229Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:40.292Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:40.332Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).TemporaryTables\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:40.371Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:40.406Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:40.435Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:40.465Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:40.473Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:40.487Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:40.498Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:40.514Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:40.552Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:40.563Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:40.593Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:40.619Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:40.643Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/Flatten.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:28:40.683Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:29:04.442Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:29:04.504Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:29:04.578Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:29:04.621Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:29:04.695Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:29:04.761Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:29:07.494Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:29:07.566Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:29:07.607Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:29:07.672Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:29:09.852Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:29:09.934Z: JOB_MESSAGE_DEBUG: Executing success step success44\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:29:10.017Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:29:10.072Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:29:10.099Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:29:53.341Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:29:53.386Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:29:53.427Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_13_23_57-6410512273965944224 is in state JOB_STATE_DONE\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp_ecfjjmy', 'apache-beam==2.24.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp_ecfjjmy', 'apache-beam==2.24.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "INFO:root:Using Python SDK docker image: apache/beam_python3.7_sdk:2.24.0. If the image is not available at local, we will try to pull from hub.docker.com\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Defaulting to the temp_location as staging_location: gs://kbisong-blessings/temp\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603485013.632175/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603485013.632175/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603485013.632175/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603485013.632175/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603485013.632175/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603485013.632175/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl in 0 seconds.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-10-23T20:30:16.992281Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-10-23_13_30_16-1891045368412670403'\n",
      " location: 'us-central1'\n",
      " name: 'kbisong-big-data'\n",
      " projectId: 'qualified-root-293322'\n",
      " stageStates: []\n",
      " startTime: '2020-10-23T20:30:16.992281Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-10-23_13_30_16-1891045368412670403]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2020-10-23_13_30_16-1891045368412670403\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-central1/2020-10-23_13_30_16-1891045368412670403?project=qualified-root-293322\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_13_30_16-1891045368412670403 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:16.094Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-10-23_13_30_16-1891045368412670403.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:16.094Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-10-23_13_30_16-1891045368412670403. The number of workers will be between 1 and 24.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:20.895Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-b.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:21.802Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:21.850Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:21.881Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:21.916Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:21.957Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:21.990Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.127Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.181Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.208Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17 for input s11.WrittenFiles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.240Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion, into producer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.262Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.297Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.332Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles) into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.356Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.383Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.418Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17-u32 for input s18.None-c30\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.443Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion/Unzipped-1, into producer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.467Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround into WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.491Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify into WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.529Z: JOB_MESSAGE_DETAILED: Fusing consumer String To BigQuery Row into ReadFile/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.564Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal into String To BigQuery Row\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.599Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination into WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.635Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile) into WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.662Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.700Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.737Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.772Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.811Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.845Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile into WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.872Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.908Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.941Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:22.975Z: JOB_MESSAGE_DETAILED: Fusing siblings WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs and WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:23.012Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs) into WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:23.035Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables into WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:23.068Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:23.126Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:23.149Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:23.174Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:23.211Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:23.244Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:23.288Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:23.330Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:23.371Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:23.422Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:23.821Z: JOB_MESSAGE_DEBUG: Executing wait step start46\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:23.902Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:23.928Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:23.939Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:23.949Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:23.971Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-b...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:23.982Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:24.015Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:24.018Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:24.031Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:24.031Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:24.059Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:24.087Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:24.116Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:24.183Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:30:24.228Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_13_30_16-1891045368412670403 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:31:19.854Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:33:04.324Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:33:04.354Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:33:59.726Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:34:00.097Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:34:00.125Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:34:00.203Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:34:00.367Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:34:00.420Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:34:00.446Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:34:00.482Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:34:00.522Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:34:00.539Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:34:00.541Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:34:00.579Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:34:00.584Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:34:00.605Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:34:00.624Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:34:00.649Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:34:00.654Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:34:00.711Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:34:00.762Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:34:00.794Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:34:00.856Z: JOB_MESSAGE_BASIC: Executing operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:20.380Z: JOB_MESSAGE_BASIC: Finished operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:20.447Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:20.500Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:20.572Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:21.976Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:22.092Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:22.145Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:22.239Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:32.531Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:32.601Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:32.621Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).TemporaryTables\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:32.643Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:32.702Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:32.747Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:32.750Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:32.778Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:32.814Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:32.814Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:32.819Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:32.876Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:32.909Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:32.912Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:32.943Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:32.979Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/Flatten.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:35:33.013Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:36:07.953Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:36:08.044Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:36:08.134Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:36:08.226Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:36:08.304Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:36:08.404Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:36:10.300Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:36:10.377Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:36:10.434Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:36:10.509Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:36:11.412Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:36:11.499Z: JOB_MESSAGE_DEBUG: Executing success step success44\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:36:11.598Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:36:11.649Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:36:11.675Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:03.648Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:03.709Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:03.743Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_13_30_16-1891045368412670403 is in state JOB_STATE_DONE\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpb3jgv1n6', 'apache-beam==2.24.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpb3jgv1n6', 'apache-beam==2.24.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "INFO:root:Using Python SDK docker image: apache/beam_python3.7_sdk:2.24.0. If the image is not available at local, we will try to pull from hub.docker.com\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Defaulting to the temp_location as staging_location: gs://kbisong-blessings/temp\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603485442.377496/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603485442.377496/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603485442.377496/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603485442.377496/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603485442.377496/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603485442.377496/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl in 0 seconds.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-10-23T20:37:25.886440Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-10-23_13_37_24-2562937757803732246'\n",
      " location: 'us-central1'\n",
      " name: 'kbisong-big-data'\n",
      " projectId: 'qualified-root-293322'\n",
      " stageStates: []\n",
      " startTime: '2020-10-23T20:37:25.886440Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-10-23_13_37_24-2562937757803732246]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2020-10-23_13_37_24-2562937757803732246\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-central1/2020-10-23_13_37_24-2562937757803732246?project=qualified-root-293322\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_13_37_24-2562937757803732246 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:24.765Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-10-23_13_37_24-2562937757803732246. The number of workers will be between 1 and 24.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:24.765Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-10-23_13_37_24-2562937757803732246.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:29.302Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-b.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:29.995Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:30.046Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:30.091Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:30.135Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:30.185Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:30.221Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:30.365Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:30.426Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:30.469Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17 for input s11.WrittenFiles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:30.512Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion, into producer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:30.543Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:30.575Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:30.611Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles) into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:30.646Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:30.681Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:30.719Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17-u32 for input s18.None-c30\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:30.751Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion/Unzipped-1, into producer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:30.810Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround into WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:30.844Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify into WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:30.891Z: JOB_MESSAGE_DETAILED: Fusing consumer String To BigQuery Row into ReadFile/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:30.920Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal into String To BigQuery Row\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:30.955Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination into WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:31.002Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile) into WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:31.050Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:31.088Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:31.129Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:31.174Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:31.210Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:31.244Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile into WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:31.277Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:31.304Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:31.351Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:31.385Z: JOB_MESSAGE_DETAILED: Fusing siblings WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs and WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:31.428Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs) into WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:31.466Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables into WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:31.502Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:31.536Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:31.572Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:31.605Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:31.640Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:31.673Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:31.714Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:31.758Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:31.793Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:31.833Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:32.075Z: JOB_MESSAGE_DEBUG: Executing wait step start46\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:32.154Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:32.191Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:32.214Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:32.224Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:32.256Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-b...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:32.273Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:32.309Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:32.312Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:32.326Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:32.326Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:32.381Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:32.415Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:32.451Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:32.489Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:37:32.530Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_13_37_24-2562937757803732246 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:38:01.019Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:39:53.747Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:39:53.786Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:40:47.856Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:40:47.954Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:40:47.988Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:40:48.021Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:40:48.072Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:40:48.106Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:40:48.150Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:40:48.162Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:40:48.188Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:40:48.220Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:40:48.222Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:40:48.240Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:40:48.261Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:40:48.298Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:40:48.305Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:40:48.331Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:40:48.348Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:40:48.384Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:40:48.427Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:40:48.469Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:40:48.558Z: JOB_MESSAGE_BASIC: Executing operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:41:56.331Z: JOB_MESSAGE_BASIC: Finished operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:41:56.404Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:41:56.459Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:41:56.531Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:41:56.889Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:41:56.980Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:41:57.042Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:41:57.124Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:06.023Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:06.094Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:06.129Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).TemporaryTables\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:06.164Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:06.201Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:06.231Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:06.258Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:06.280Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:06.292Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:06.296Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:06.325Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:06.370Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:06.381Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:06.425Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:06.457Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:06.490Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/Flatten.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:06.542Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:40.215Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:40.314Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:40.406Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:40.477Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:40.553Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:40.623Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:43.399Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:43.485Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:43.542Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:43.625Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:44.840Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:44.938Z: JOB_MESSAGE_DEBUG: Executing success step success44\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:45.064Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:45.132Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:42:45.167Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:43:38.807Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:43:38.866Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:43:38.913Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_13_37_24-2562937757803732246 is in state JOB_STATE_DONE\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpbqrq6nc4', 'apache-beam==2.24.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpbqrq6nc4', 'apache-beam==2.24.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "INFO:root:Using Python SDK docker image: apache/beam_python3.7_sdk:2.24.0. If the image is not available at local, we will try to pull from hub.docker.com\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Defaulting to the temp_location as staging_location: gs://kbisong-blessings/temp\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603485841.432400/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603485841.432400/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603485841.432400/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603485841.432400/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603485841.432400/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603485841.432400/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl in 0 seconds.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-10-23T20:44:04.543900Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-10-23_13_44_03-14205667959099393348'\n",
      " location: 'us-central1'\n",
      " name: 'kbisong-big-data'\n",
      " projectId: 'qualified-root-293322'\n",
      " stageStates: []\n",
      " startTime: '2020-10-23T20:44:04.543900Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-10-23_13_44_03-14205667959099393348]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2020-10-23_13_44_03-14205667959099393348\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-central1/2020-10-23_13_44_03-14205667959099393348?project=qualified-root-293322\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_13_44_03-14205667959099393348 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:03.591Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-10-23_13_44_03-14205667959099393348.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:03.591Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-10-23_13_44_03-14205667959099393348. The number of workers will be between 1 and 24.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:08.413Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-b.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:09.045Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:09.081Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:09.105Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:09.142Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:09.185Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:09.228Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:09.384Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:09.434Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:09.481Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17 for input s11.WrittenFiles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:09.513Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion, into producer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:09.550Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:09.585Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:09.616Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles) into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:09.645Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:09.677Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:09.718Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17-u32 for input s18.None-c30\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:09.763Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion/Unzipped-1, into producer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:09.794Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround into WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:09.838Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify into WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:09.872Z: JOB_MESSAGE_DETAILED: Fusing consumer String To BigQuery Row into ReadFile/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:09.920Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal into String To BigQuery Row\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:09.950Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination into WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.005Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile) into WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.037Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.069Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.103Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.142Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.173Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.206Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile into WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.242Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.273Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.300Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.329Z: JOB_MESSAGE_DETAILED: Fusing siblings WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs and WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.374Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs) into WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.402Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables into WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.441Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.468Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.497Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.529Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.565Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.588Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.620Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.657Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.696Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.727Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:10.925Z: JOB_MESSAGE_DEBUG: Executing wait step start46\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:11Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:11.039Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:11.043Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:11.069Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-b...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:11.079Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:11.116Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:11.135Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:11.135Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:11.160Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:11.163Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:11.208Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:11.219Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:11.244Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:11.278Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:11.321Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_13_44_03-14205667959099393348 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:44:40.270Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:46:35.081Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:46:35.130Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:47:36.471Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:47:36.542Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:47:36.580Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:47:36.617Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:47:36.656Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:47:36.689Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:47:36.732Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:47:36.739Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:47:36.768Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:47:36.785Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:47:36.804Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:47:36.825Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:47:36.835Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:47:36.874Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:47:36.900Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:47:36.907Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:47:36.909Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:47:36.945Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:47:37.002Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:47:37.030Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:47:37.117Z: JOB_MESSAGE_BASIC: Executing operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:03.152Z: JOB_MESSAGE_BASIC: Finished operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:03.227Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:03.284Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:03.344Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:05.400Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:05.475Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:05.532Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:05.596Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:17.296Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:17.372Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:17.413Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).TemporaryTables\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:17.446Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:17.483Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:17.518Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:17.550Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:17.554Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:17.582Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:17.586Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:17.606Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:17.621Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:17.655Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:17.661Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:17.701Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:17.735Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/Flatten.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:49:17.774Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:50:01.689Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:50:01.773Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:50:01.852Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:50:01.913Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:50:01.989Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:50:02.068Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:50:03.782Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:50:03.909Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:50:03.955Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:50:04.021Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:50:04.921Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:50:05.003Z: JOB_MESSAGE_DEBUG: Executing success step success44\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:50:05.071Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:50:05.127Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:50:05.162Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:04.354Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:04.408Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:04.441Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_13_44_03-14205667959099393348 is in state JOB_STATE_DONE\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp1v3_m7v3', 'apache-beam==2.24.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp1v3_m7v3', 'apache-beam==2.24.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "INFO:root:Using Python SDK docker image: apache/beam_python3.7_sdk:2.24.0. If the image is not available at local, we will try to pull from hub.docker.com\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Defaulting to the temp_location as staging_location: gs://kbisong-blessings/temp\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603486285.034216/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603486285.034216/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603486285.034216/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603486285.034216/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603486285.034216/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://kbisong-blessings/temp/kbisong-big-data.1603486285.034216/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl in 0 seconds.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-10-23T20:51:28.066839Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-10-23_13_51_27-1059630107670106145'\n",
      " location: 'us-central1'\n",
      " name: 'kbisong-big-data'\n",
      " projectId: 'qualified-root-293322'\n",
      " stageStates: []\n",
      " startTime: '2020-10-23T20:51:28.066839Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-10-23_13_51_27-1059630107670106145]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2020-10-23_13_51_27-1059630107670106145\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-central1/2020-10-23_13_51_27-1059630107670106145?project=qualified-root-293322\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_13_51_27-1059630107670106145 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:27.191Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-10-23_13_51_27-1059630107670106145.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:27.191Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-10-23_13_51_27-1059630107670106145. The number of workers will be between 1 and 24.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:31.400Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-b.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:32.204Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:32.257Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:32.294Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:32.408Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:32.471Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:32.510Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:32.654Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:32.714Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:32.750Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17 for input s11.WrittenFiles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:32.784Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion, into producer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:32.811Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:32.850Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:32.886Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles) into WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:32.928Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:32.961Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.001Z: JOB_MESSAGE_DETAILED: Unzipping flatten s17-u32 for input s18.None-c30\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.035Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify, through flatten WriteBiqQuery/BigQueryBatchFileLoads/DestinationFilesUnion/Unzipped-1, into producer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.093Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround into WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.130Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify into WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.167Z: JOB_MESSAGE_DETAILED: Fusing consumer String To BigQuery Row into ReadFile/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.201Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal into String To BigQuery Row\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.239Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination into WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.275Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile) into WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.318Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations) into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.358Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify into WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.401Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.433Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.461Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber into WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.505Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile into WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.553Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.603Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.654Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix into WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.695Z: JOB_MESSAGE_DETAILED: Fusing siblings WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs and WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.737Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs) into WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.788Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables into WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.830Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.886Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.937Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:33.976Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:34.026Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:34.066Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete into WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:34.124Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:34.199Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:34.238Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:34.274Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:34.724Z: JOB_MESSAGE_DEBUG: Executing wait step start46\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:34.818Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:34.876Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:34.887Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:34.922Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:34.933Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-b...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:34.956Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:35.005Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:35.010Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:35.067Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:35.067Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:35.106Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:35.160Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:35.205Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:35.253Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:51:35.286Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_13_51_27-1059630107670106145 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:52:03.090Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:53:39.766Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:53:39.802Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:35.963Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:36.042Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:36.075Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/LoadJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:36.114Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/CopyJobNamePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:36.152Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/GenerateFilePrefix.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:36.176Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:36.202Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:36.224Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:36.232Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:36.258Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:36.305Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:36.319Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:36.355Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:36.378Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:36.410Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:36.439Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:36.455Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:36.522Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:36.567Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:36.614Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:36.708Z: JOB_MESSAGE_BASIC: Executing operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:55.497Z: JOB_MESSAGE_BASIC: Finished operation ReadFile/Read+String To BigQuery Row+WriteBiqQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteBiqQuery/BigQueryBatchFileLoads/AppendDestination+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:55.587Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:55.646Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:55.744Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:58.783Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/DropShardNumber+WriteBiqQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteBiqQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:58.862Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:58.924Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:54:58.997Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:09.784Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteBiqQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:09.865Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:09.897Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).TemporaryTables\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:09.935Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:09.968Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:09.993Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:10.030Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:10.039Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:10.060Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:10.061Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:10.091Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:10.108Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:10.149Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:10.150Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:10.196Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:10.237Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/Flatten.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:10.304Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:13Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:13.125Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:13.224Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:13.315Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:13.415Z: JOB_MESSAGE_DEBUG: Value \"WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:13.547Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:15.581Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:15.709Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:15.807Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:15.947Z: JOB_MESSAGE_BASIC: Executing operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:18.296Z: JOB_MESSAGE_BASIC: Finished operation WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames+WriteBiqQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:18.504Z: JOB_MESSAGE_DEBUG: Executing success step success44\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:18.768Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:18.973Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:55:19.019Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:56:16.337Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:56:16.442Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-10-23T20:56:16.505Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-10-23_13_51_27-1059630107670106145 is in state JOB_STATE_DONE\n"
     ]
    }
   ],
   "source": [
    "for key, value in table.items():\n",
    "    for source in value:\n",
    "        dataflow_runner(key, source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkGQoKhiJB8v"
   },
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "tFmy4JfBXjKP",
    "a5hIJAo1bNWv"
   ],
   "name": "karis_dataflow.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m58"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
